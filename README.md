# Machine-Big-Prediction-Model
Techniques for training large neural networks
In recent years, many NLP benchmarks have seen better results with larger pre-training. Training large deep neural networks is challenging because it requires a lot of GPU memory and long training times.

However, the memory of a single GPU worker is limited, and the size of many large models has exceeded the capabilities of a single GPU. There are several parallel paradigms that can train models across multiple GPUs, and there are various model architectures and memory-saving designs that can help train _very large_ neural networks.

This blog was written by a former Open AI employee, and I compiled it from it.
